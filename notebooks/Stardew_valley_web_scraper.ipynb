{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnWe214yyCTzROIkex1qvU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cecax27/DS-Stardew-Valley-Crops-Profit/blob/main/notebooks/Stardew_valley_web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraper on Stardew Valley wiki\n",
        "\n",
        "This notebook is to make web scraping on the Stardew Valley Wiki. We need to extract information about the crops, and the products that you can create with them.\n",
        "\n",
        "The url of the website is https://stardewvalleywiki.com. You can visit it to find a lot of information about the game.\n",
        "\n",
        "## Importing libraries"
      ],
      "metadata": {
        "id": "Hu9Ot_ZvI-TD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uyy0n8B6GoY0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring constants"
      ],
      "metadata": {
        "id": "KoHjNG-UZt0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "home_url = 'https://stardewvalleywiki.com'\n",
        "crops_url = 'https://stardewvalleywiki.com/Crops'"
      ],
      "metadata": {
        "id": "2GHdP9dL7pT_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get crops urls list"
      ],
      "metadata": {
        "id": "A36oqlPxJRVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the request\n",
        "crops_page = requests.get(crops_url)\n",
        "\n",
        "# Pasing the HTML with BeatifulSoup\n",
        "crops_soup = BeautifulSoup(crops_page.text, 'lxml')\n",
        "\n",
        "#Finding the urls\n",
        "crops_names = crops_soup.find('div', attrs = {'class' : 'mw-parser-output'}).find_all('h3')[10:-1]\n",
        "\n",
        "#Making a links list\n",
        "crops_links_list = [home_url + crop_name.find_all('a')[1].get('href') for crop_name in crops_names]\n",
        "crops_names_list = [crop_name.get_text().strip() for crop_name in crops_names]\n",
        "\n",
        "print(f'{len(crops_links_list)} links founded')"
      ],
      "metadata": {
        "id": "E66wbo5eHOZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf05ad66-d535-4901-b287-97e64448e5cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43 links founded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring functions to extract data"
      ],
      "metadata": {
        "id": "SB4DRDonqz8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract information about the seed\n",
        "\n",
        "def get_seed_information(link) -> dict:\n",
        "\n",
        "  # Getting data to work\n",
        "  try:\n",
        "    s = requests.get(link)\n",
        "    raw_information = BeautifulSoup(s.text, 'lxml')\n",
        "  except Exception as e:\n",
        "    print('An error ocurred making the request:')\n",
        "    print(e)\n",
        "    print('\\n')\n",
        "    return dict()\n",
        "\n",
        "  # Dictionary to save the data\n",
        "  price_dict = dict()\n",
        "\n",
        "  # Finding data\n",
        "  for row in raw_information.find('table').find_all('tr'):\n",
        "\n",
        "    title = row.find('td').get_text().strip().replace(':', '').lower().replace(' ', '_')\n",
        "\n",
        "    if title == 'general_store':\n",
        "      try:\n",
        "        price_dict[title+'_price'] = int(row.find_all('td')[1].find_all('span')[1].get_text()[:-1])\n",
        "      except:\n",
        "        price_dict[title+'_price'] = np.nan\n",
        "\n",
        "    elif title == 'jojamart':\n",
        "      try:\n",
        "        price_dict[title+'_price'] = int(row.find_all('td')[1].find_all('span')[1].get_text()[:-1])\n",
        "      except:\n",
        "        price_dict[title+'_price'] = np.nan\n",
        "\n",
        "    elif title == 'oasis':\n",
        "      price_dict[title+'_price'] = int(row.find_all('td')[1].find_all('span')[1].get_text()[:-1])\n",
        "\n",
        "    elif title == 'traveling_cart':\n",
        "      try:\n",
        "        price_dict[title+'_price_min'] = int(row.find_all('td')[1].get_text().split('\"')[-1].split('g')[0].replace(',','').split('–')[0])\n",
        "        price_dict[title+'_price_max'] = int(row.find_all('td')[1].get_text().split('\"')[-1].split('g')[0].replace(',','').split('–')[1])\n",
        "      except:\n",
        "        price_dict[title+'_price_min'] = np.nan\n",
        "        price_dict[title+'_price_max'] = np.nan\n",
        "\n",
        "    elif title == 'night_market(winter_15)':\n",
        "      price_dict[title.split('(')[0]+'_price'] = int(row.find_all('td')[1].find_all('span')[1].get_text()[:-1])\n",
        "\n",
        "    elif title == 'other':\n",
        "      price_dict[row.find_all('td')[1].find('a').get_text().strip().lower().replace(' ', '_')+'_price'] = int(row.find_all('td')[1].find_all('span')[-1].get_text().split('g')[0])\n",
        "\n",
        "  return price_dict"
      ],
      "metadata": {
        "id": "KUphWuyLrqpo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "pprint(get_seed_information('https://stardewvalleywiki.com/Starfruit_Seeds'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcVb7KgVsOFp",
        "outputId": "9c2a38c5-bb90-4801-81fd-d7f462e3ce92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'general_store_price': nan,\n",
            " 'jojamart_price': nan,\n",
            " 'oasis_price': 400,\n",
            " 'traveling_cart_price_max': 1000,\n",
            " 'traveling_cart_price_min': 600}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_crop_information(link) -> dict:\n",
        "\n",
        "  # Getting data to work\n",
        "  try:\n",
        "    s = requests.get(link)\n",
        "    raw_information = BeautifulSoup(s.text, 'lxml').find('table').find_all('tr')\n",
        "  except Exception as e:\n",
        "    print('An error ocurred making the request:')\n",
        "    print(e)\n",
        "    print('\\n')\n",
        "    return dict()\n",
        "\n",
        "  # Constants\n",
        "  prices_sufix = ['regular', 'silver', 'gold', 'iridium']\n",
        "\n",
        "  # Dictionary to save the data\n",
        "  inf_dict = {\n",
        "      'name' : raw_information[0].get_text().strip(),\n",
        "      'description' : raw_information[2].get_text().strip()\n",
        "  }\n",
        "\n",
        "  # Finding data\n",
        "  for data in raw_information[4:]:\n",
        "\n",
        "    field = data.find('td').get_text().strip().lower().replace(' ', '_')\n",
        "\n",
        "    if '\\t' in field or ':' in field or '/' in field or field == '' or field[0].isdigit():\n",
        "      continue\n",
        "\n",
        "    if field == 'sell_price' or field == 'sell_prices':\n",
        "      try:\n",
        "        prices = data.find('table').find_all('td')\n",
        "        prices = list(map(lambda i: i.get_text().split('g')[0], prices))[1:]\n",
        "        prices = list(filter(lambda i: i != '', prices))\n",
        "        prices = list(map(lambda i: int(i.replace(',', '')), prices))\n",
        "        for index, price in enumerate(prices):\n",
        "          inf_dict[field+'_'+prices_sufix[index]] = price\n",
        "        continue\n",
        "      except Exception as e:\n",
        "        try:\n",
        "          index = raw_information.index(data) + 2\n",
        "          prices = raw_information[index].find('table').find_all('td')\n",
        "          prices = list(map(lambda i: i.get_text().split('g')[0].replace(',',''), prices))[1:]\n",
        "          prices = list(filter(lambda i: i != '', prices))\n",
        "          prices = list(map(lambda i: int(i), prices))\n",
        "          for index, price in enumerate(prices):\n",
        "            inf_dict[field+'_'+prices_sufix[index]] = price\n",
        "          continue\n",
        "        except Exception as e:\n",
        "          td = data.find_all('td')\n",
        "          if td:\n",
        "            span = td[-1].find_all('span')\n",
        "            if span:\n",
        "              price = int(span[-1].get_text().split('g')[0].strip())\n",
        "              for index in range(4):\n",
        "                inf_dict[field+'_'+prices_sufix[index]] = price\n",
        "              continue\n",
        "          print(f'An error ocurred extracting the data \"{field}\" from {link}:')\n",
        "          print(e)\n",
        "          print('\\n')\n",
        "        continue\n",
        "\n",
        "    if field == 'seed':\n",
        "      try:\n",
        "        seed_link = data.find_all('td')[1].find('a').get('href')\n",
        "        if seed_link:\n",
        "          url = home_url + seed_link\n",
        "          inf_dict.update(get_seed_information(url))\n",
        "      except Exception as e:\n",
        "        print(f'An error ocurred extracting the data \"{field}\" from {link}:')\n",
        "        print(e)\n",
        "        print('\\n')\n",
        "    try:\n",
        "      inf_dict[field] = data.find_all('td')[1].get_text().strip().replace(' • ', ',')\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return inf_dict"
      ],
      "metadata": {
        "id": "9H2Jf8TX-yz1"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "pprint(get_crop_information('https://stardewvalleywiki.com/Coffee_Bean'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG5Mp31KxL3A",
        "outputId": "8ce295ce-e2de-41e9-86c0-fb8b11d5a947"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "{'artisan_sell_price': '150g',\n",
            " 'description': 'Plant in spring or summer to grow a coffee plant. Place five '\n",
            "                'beans in a keg to make coffee.',\n",
            " 'energy': 'Inedible',\n",
            " 'growth_time': '10 days',\n",
            " 'name': 'Coffee Bean',\n",
            " 'season': 'Spring,\\xa0Summer',\n",
            " 'seed': 'Coffee Bean',\n",
            " 'sell_price_gold': 22,\n",
            " 'sell_price_iridium': 30,\n",
            " 'sell_price_regular': 15,\n",
            " 'sell_price_silver': 18,\n",
            " 'source': 'Dust Sprite,Traveling Cart',\n",
            " 'xp': '4 Farming XP per harvest'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting all the data\n",
        "\n",
        "I'll call my extract data functions with my links list. Also I'll save the data in a Pandas DataFrame."
      ],
      "metadata": {
        "id": "EmC2ozlX6Pot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data\n",
        "df_data = [get_crop_information(link) for link in crops_links_list]\n",
        "\n",
        "# Extracting DataFrame columns\n",
        "columns = set()\n",
        "for sublist in [list(item.keys()) for item in df_data]:\n",
        "  for element in sublist:\n",
        "    columns.add(element)\n",
        "columns = list(columns)\n",
        "\n",
        "# Making the DataFrame\n",
        "df = pd.DataFrame(df_data, columns = columns)\n",
        "\n",
        "print(f'DataFrame created with {df.shape[0]} rows and {df.shape[1]} columns\\n')\n",
        "print('Columns:')\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "fWqJCKcv_r8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'DataFrame created with {df.shape[0]} rows and {df.shape[1]} columns\\n')\n",
        "print('Columns:')\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpmq_GC4Mzxl",
        "outputId": "74c6cfad-16c1-43b6-b61b-e91bec1607df"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame created with 43 rows and 25 columns\n",
            "\n",
            "Columns:\n",
            "jojamart_price              float64\n",
            "sell_prices_iridium         float64\n",
            "night_market_price          float64\n",
            "artisan_sell_price           object\n",
            "name                         object\n",
            "xp                           object\n",
            "season                       object\n",
            "general_store_price         float64\n",
            "sell_price_regular          float64\n",
            "traveling_cart_price_min    float64\n",
            "source                       object\n",
            "growth_time                  object\n",
            "sell_prices_regular         float64\n",
            "egg_festival_price          float64\n",
            "traveling_cart_price_max    float64\n",
            "sell_price_iridium          float64\n",
            "base                         object\n",
            "sell_prices_silver          float64\n",
            "sell_price_silver           float64\n",
            "description                  object\n",
            "energy                       object\n",
            "seed                         object\n",
            "sell_prices_gold            float64\n",
            "oasis_price                 float64\n",
            "sell_price_gold             float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I'll save my data in crops_raw_data.csv file."
      ],
      "metadata": {
        "id": "mOzO5BRKNika"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('crops_raw_data.csv')"
      ],
      "metadata": {
        "id": "hmPGksHoNtiT"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}